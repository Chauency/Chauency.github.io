<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>
---
layout:     post
title:      AdaBoost速记
subtitle:   AdaBoost的算法流程
date:       2018-09-16
author:     Chauency
catalog: 	 true
tags:
    - Adaboost
    - Boost
---

> *前记：本年度最强台风“山竹”来袭，窗外狂风怒号，昏天黑地。内心百无聊懒，只好总结一下最近看的AdaBoost。--2019-09-16*

## 集成学习简介

“先训练若干个弱学习器，然后通过一定的结合策略，最后形成一个强的学习器，达到博采众长的目的”，这就是集成学习方法的主要思想。集成学习方法通常要解决两个大类问题：

1. 如何学习到弱学习器
2. 弱学习器间如何结合

按照弱学习器间是否存在依赖关系，可以把集成学习方法分为两类：

1. 弱学习器间**存在依赖**关系，各弱学习器需要**串行**生成，如Boosting算法。
2. 弱学习器间**不存在依赖**关系，各弱学习器可以**并行**生成，例如Bagging和随机森林等。

本文将要介绍的AdaBoost便是Boosting中的一种。
## AdaBoost
### AdaBoost算法概述
在训练好的某个弱学习后，增大被误分的样本的权重（同时减少分正确分类的样本的权重），使得这些样本在下一个弱分类器中起到更大的作用（或者说这些样本再次被分错将使得被惩罚项更大），最后就是使用一定的结合方法把这些弱学习器结合起来形成一个强学习器。

### AdaBoost的四个问题：

1. 如何计算分类误差率
2. 如何更新每个样本的权重
3. 如何计算每个弱分类器的权重
4. 如何结合弱分类器

### AdaBoost算法流程
输入：训练样本$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中$x_i\in\mathcal{X}\subset \textbf{R}^n$，$y_i\in\mathcal{Y} = \{-1,+1\}$；弱学习算法；  
输出：最终分类器$G(x)$.

1. 初始化样本权重  
$D_1 = (w_{1,1}, w_{1,2}, ...w_{1,N}),\;\; w_{1i}=\frac{1}{N};\;\; i =1,2,\cdots,N
$

2. 对$m = 1, 2, \cdots, M$  
a)使用具有权值分布$D_m$的样本集来训练第$m$个弱学习器$G_m(x)$  
b)计算$G_m(x)$的分类误差率 $$e_m = \sum\limits_{i=1}^{m}w_{mi}I(G_m(x_i) \neq y_i)$$
**Remark**：
 -  <font color=red> 所谓的权重只是影响了计算分类误差率，并没有对样本值发生改变</font>
 -  <font color=red> 分类误差率实际上是所有被误分样本的权重之和 </font> 
 
 c)计算弱分类器的系数$\alpha_m = \frac{1}{2}\log\frac{1-e_m}{e_m}$  
 **Remark**:  
 <font color=red> 只考虑$e_m\leq0.5$的情况（若$e_m>0.5$说明这个弱分类器比随机分类还差，可弃之而后快），则有$\alpha\geq0$，且$e_m$越小，$\alpha_m$越大，即分类效果好的弱分类器最后的“话事权”也大一点。</font>   
 d) 更新样本权值分布$$D_{m+1} = (w_{m+1,1}, w_{m+1,2}, ...w_{m+1,N})$$$$w_{m+1, i} = \frac{w_{m, i}}{Z_m}\exp(-\alpha_my_iG_m(x_i)), i = 1, 2,\cdots, N,$$其中$Z_m$是规范化因子（使得更新后的权值之后为1）$$Z_m = \sum_{i = 1}^{N}w_{m,i}\exp(-\alpha_my_iG_m(x_i))$$  
 **Remark**：  
 <font color=red>考虑$\exp(-\alpha_my_iG_m(x_i))$，当第$m$个弱分类器对第$i$各样本正确分类的时候，$-\alpha_my_iG_m(x_i)\leq 0 $，使得$\exp(-\alpha_my_iG_m(x_i))\leq 1$；反之，$\exp(-\alpha_my_iG_m(x_i))\geq 1$，这恰恰是我们想要的效果：降低被正确分类的样本的权重，同时提高被误分样本的权重</font> 
 
3. 将弱分类器的线形组合作为最终的强分类器
 $$G(x) = \text{sign}\left(\sum_{m = 1}^{M}\alpha_mG_m(x)\right)$$
 
 <font color=red> </font>

## AdaBoost训练误差分析
参考《统计学习方法》--李航
## AdaBoost和前向加法模型
参考《统计学习方法》--李航。需要知道AdaBoost就是由基本分类器组成的加法模型，其损失函数是指数函数。
## AdaBoost优缺点
优点： 

1. 具有较高的精度
2. 可以组合不同的分类算法
3. 充分考虑了弱分类器的权重
4. 结构简单，可解释性强

缺点：

1. 串联结构，训练时间长
2. 对异常值敏感，迭代会使得异常样本的权值变得很大，从而影响最终的分类精度


