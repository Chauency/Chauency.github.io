---
layout:     post
title:      AdaBoost速记
subtitle:   AdaBoost的算法流程
date:       2018-09-16
author:     Chauency
catalog: 	 true
tags:
    - Adaboost
    - Boost
---

> *前记：本年度最强台风“山竹”来袭，窗外狂风怒号，昏天黑地。内心百无聊懒，只好总结一下最近看的AdaBoost。--2019-09-16*

## 集成学习简介

“先训练若干个弱学习器，然后通过一定的结合策略，最后形成一个强的学习器，达到博采众长的目的”，这就是集成学习方法的主要思想。集成学习方法通常要解决两个大类问题：

1. 如何学习到弱学习器
2. 弱学习器间如何结合

按照弱学习器间是否存在依赖关系，可以把集成学习方法分为两类：

1. 弱学习器间**存在依赖**关系，各弱学习器需要**串行**生成，如Boosting算法。
2. 弱学习器间**不存在依赖**关系，各弱学习器可以**并行**生成，例如Bagging和随机森林等。

本文将要介绍的AdaBoost便是Boosting中的一种。
## AdaBoost
### AdaBoost算法概述
在训练好的某个弱学习后，增大被误分的样本的权重（同时减少分正确分类的样本的权重），使得这些样本在下一个弱分类器中起到更大的作用（或者说这些样本再次被分错将使得被惩罚项更大），最后就是使用一定的结合方法把这些弱学习器结合起来形成一个强学习器。

### AdaBoost需要解决的四个问题：（⚠️敲黑板⚠️）

1. 如何计算分类误差率
2. 如何更新每个样本的权重
3. 如何计算每个弱分类器的权重
4. 如何结合弱分类器

### AdaBoost算法流程

